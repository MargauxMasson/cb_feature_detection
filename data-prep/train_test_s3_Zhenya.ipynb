{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_bucket = s3.Bucket('canopy-production-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "chips = []\n",
    "for obj in pc_bucket.objects.all():\n",
    "    if 'yes' in obj.key:\n",
    "        chips.append(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for chip in chips[0:4]:\n",
    "    id = chip.key.split(\"/\")[4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_file = \"train_test_polygons.json\"\n",
    "\n",
    "with open(j_file, 'r') as j:\n",
    "    train_test = json.loads(j.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_test[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_test[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chips/cloudfree-merge-polygons/yes/Habitation/100/101/101_1000_3000.tif'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chips[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Chip List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('yes_chips_s3.json', 'w') as fp:\n",
    "    json.dump(chips, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chips/cloudfree-merge-polygons/yes/Habitation/100/101/101_1000_3000.tif',\n",
       " 'chips/cloudfree-merge-polygons/yes/Habitation/100/101/101_1000_3100.tif',\n",
       " 'chips/cloudfree-merge-polygons/yes/Habitation/100/101/101_1000_400.tif']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chips[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_s3_copy(chips, j_file, bucket_name='canopy-production-ml',\n",
    "                       base_path='chips/cloudfree-merge-polygons/split/'):\n",
    "    \n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    with open(j_file, 'r') as j:\n",
    "        \n",
    "        train_test_file = json.loads(j.read())\n",
    "        \n",
    "#     for split in list(train_test.keys()):\n",
    "        \n",
    "#         for polygon in train_test[split]:\n",
    "\n",
    "    length = len(chips)\n",
    "            \n",
    "    for i, chip in enumerate(chips, 1):\n",
    "        print(f'Processing chip {i} of {length}', end='\\r', flush=True)\n",
    "\n",
    "        CopySource = {\n",
    "            'Bucket': bucket_name,\n",
    "            'Key': chip\n",
    "        }\n",
    "\n",
    "        polygon_id = int(chip.split(\"/\")[5])\n",
    "\n",
    "        filename = chip.split('/')[-1]\n",
    "\n",
    "        if polygon_id in train_test_file[\"test\"]:\n",
    "            train_test = 'test'\n",
    "\n",
    "            #s3.Object(bucket, base_path + 'test/').copy_from(CopySource=bucket + chip.key)\n",
    "\n",
    "        else:\n",
    "            train_test = 'train'\n",
    "\n",
    "            #s3.Object(bucket, base_path + 'train_val/').copy_from(CopySource=bucket + chip.key)\n",
    "\n",
    "        new_key = f'{base_path}{train_test}/{polygon_id}/{filename}'\n",
    "\n",
    "        bucket.copy(CopySource, new_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chip 49133 of 49133\r"
     ]
    }
   ],
   "source": [
    "j_file = \"train_test_polygons.json\"\n",
    "train_test_s3_copy(chips,j_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_list(dir_name='train',bucket='canopy-production-ml'):\n",
    "    \n",
    "    pc_bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    train_chips = []\n",
    "\n",
    "    for obj in pc_bucket.objects.all():\n",
    "        \n",
    "        if dir_name in obj.key:\n",
    "            train_chips.append(obj)\n",
    "            \n",
    "    return train_chips\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chips = s3_list()\n",
    "\n",
    "# val_chips = s3_list(dir_name='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s3.ObjectSummary(bucket_name='canopy-production-ml', key='chips/cloudfree-merge-polygons/split/train/'),\n",
       " s3.ObjectSummary(bucket_name='canopy-production-ml', key='chips/cloudfree-merge-polygons/split/train/1/1_1000_1000.tif'),\n",
       " s3.ObjectSummary(bucket_name='canopy-production-ml', key='chips/cloudfree-merge-polygons/split/train/1/1_1000_1100.tif')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chips[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_val_split(train_chips,bucket_name='canopy-production-ml',\n",
    "                    in_base_path='chips/cloudfree-merge-polygons/split/',\n",
    "                    out_base_path='chips/cloudfree-merge-polygons/split/val/'):\n",
    "    \n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    train_chips = train_chips.copy()\n",
    "    \n",
    "    dir_dict = {}\n",
    "    \n",
    "    for chip_1 in train_chips:\n",
    "        \n",
    "        chip_key = chip_1.key\n",
    "        \n",
    "        poly_id = chip_key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        \n",
    "        if poly_id in dir_dict.keys():\n",
    "                \n",
    "            dir_dict[poly_id].append(chip_key)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            dir_dict[poly_id] = [chip_key]\n",
    "            \n",
    "    \n",
    "    key_num = len(dir_dict.keys())\n",
    "    \n",
    "    for i_1,key in enumerate(dir_dict.keys(),1):\n",
    "        \n",
    "        dir_len = len(dir_dict[key])\n",
    "        \n",
    "        val_slice = dir_len // 5 \n",
    "        \n",
    "        val_chips = dir_dict[key][:val_slice]\n",
    "        \n",
    "        val_num = len(val_chips)\n",
    "        \n",
    "        for i_2,val_chip_key in enumerate(val_chips,1):\n",
    "            \n",
    "            print(f\"Copying chip {i_2} of {val_num} in polygon {i_1} of {key_num}\", end='\\r', flush=True)\n",
    "            \n",
    "            CopySource = {\n",
    "                'Bucket': bucket_name,\n",
    "                'Key':val_chip_key}\n",
    "            \n",
    "            polygon_id = val_chip_key.split(\"/\")[-1].split(\"_\")[0]\n",
    "            \n",
    "            filename = val_chip_key.split('/')[-1]\n",
    "            \n",
    "            new_key = f'{out_base_path}{polygon_id}/{filename}'\n",
    "            \n",
    "            bucket.copy(CopySource, new_key)\n",
    "            \n",
    "            obj = s3.Object('canopy-production-ml', val_chip_key)\n",
    "            \n",
    "#             obj.delete()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying chip 285 of 285 in polygon 47 of 4747\r"
     ]
    }
   ],
   "source": [
    "train_val_split(train_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_chips = s3_list(dir_name='val',bucket='canopy-production-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34071"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_dir(file_list,\n",
    "             out_base_path='chips/cloudfree-merge-polygons/split/train_copy/',\n",
    "             bucket_name='canopy-production-ml'):\n",
    "\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    dir_num = len(file_list)\n",
    "\n",
    "    for i_2,chip in enumerate(file_list,1):\n",
    "\n",
    "        chip_key = chip.key\n",
    "\n",
    "        print(f\"Copying chip {i_2} of {dir_num}\", end='\\r', flush=True)\n",
    "\n",
    "        CopySource = {\n",
    "            'Bucket': 'canopy-production-ml',\n",
    "            'Key':chip_key}\n",
    "\n",
    "        polygon_id = chip_key.split(\"/\")[-1].split(\"_\")[0]\n",
    "\n",
    "        filename = chip_key.split('/')[-1]\n",
    "\n",
    "        new_key = f'{out_base_path}{polygon_id}/{filename}'\n",
    "\n",
    "        bucket.copy(CopySource, new_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying chip 21865 of 34442\r"
     ]
    }
   ],
   "source": [
    "copy_dir(train_chips[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chip in val_chips[1:]:\n",
    "    \n",
    "    obj = s3.Object('canopy-production-ml', chip.key)\n",
    "    obj.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Lst Format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dict = {\"train\": \n",
    "                   [12, 74, 7, 28, 10, 19, 9, 21, 32, 31, 64, 58, 86, 27, 77, 53, 33, 84, 30, 18, 22, 15, 25, 78, 75, 85, 89, 55, 39, 66, 44, 59, 81, 60, 92, 101], \n",
    "                   \"test\": [1, 6, 2, 72, 70, 14, 20, 5, 13, 26, 3, 11, 4, 37, 76, 69, 41, 35, 47, 46, 43, 34, 63, 61, 17, 23, 16, 24, 8, 48, 29, 68, 36, 83, 88, 42, 40, 38, 45, 71, 87, 49, 52, 82, 57, 50, 56, 80, 51, 54, 91, 99, 100, 93, 90, 94, 67, 65, 62, 73, 79]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.read_csv(\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Labels/local_multi_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lst(df_in):\n",
    "    # insert code\n",
    "    return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelling-2",
   "language": "python",
   "name": "labelling-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
